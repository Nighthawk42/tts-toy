------------------------------------
TODO
------------------------------------

style titlebar

make named tuple for replay variable

make public

    add README, add github. add github link in menu text
    make demo video, add to README
    post to reddit maybe

test other llm models more and fine-tune sys prompt if needed

enum for send_ui_message

option to verbalize user prompt; 
two independent voice settings maybe, one for user prompt and one for assistant message

[1] doublecheck possible bug:
    some chat, !clear, and then:
    what was the last thing i asked                                                                          
    <chuckle> you just asked me what the last thing you asked was. 

[2] after returning to idling computer after several hours, app had no sound output (2x). buffer underflow at least one of those times.

[3] text chunker - add second pass where it combines short sentences that will fit. probably does not enhance quality.

[3] get runtime errors on control-c - ugh, or meh?

[3] missing logic: chat request > [direct] > say stuff > chat request completes and adds to audio queue

[3] add mode to style transformer where it takes in a single color and ignores all special tags. use for user input printout

[feature] stream llm content, and dynamically chunk text as the text comes in. test using slow LLM like deepseek
[feature] only print out text chunks as they are being spoken. and maybe tween the printing of the characters linearly based on full duration.
[feature] print out llm content synchronously but "highlight" each chunk as it's being spoken

------------------------------------
MISC NOTES, operational
------------------------------------

local orpheus llm server
    my llama-server command
        C:\sd\llama.cpp\llama-server.exe -m c:\sd\models\llm\Orpheus-3b-FT-Q8_0.gguf -ngl 99 -c 4096 --host 0.0.0.0 --temp 0.5
        C:\sd\llama.cpp\llama-server.exe -m c:\sd\models\llm\orpheus-3b-0.1-ft-q4_k_m.gguf -ngl 99 -c 4096 --host 0.0.0.0 --temp 0.5

    llama-server temp setting is overridden by request setting fyi, as you would expect

    lm studio seems to behave fine as well

single-file gguf version of the original hf model?

    had to run llama.cpp convert script from wsl (sentencepiece install error)

    python convert_hf_to_gguf.py /mnt/c/sd/models/llm/orpheus_full_temp --outfile /mnt/c/sd/models/llm/orpheus_full_temp/combined

    error: cannot find tokenizer.model

    simply copying tokenizer.json to tokenizer.model didn't cut it

    could not figure out :/

orpheus 'idempotency'

    temp 0 + top_p 0 = kind-of-almost. repeating same prompt multiple times helps as well.

performance notes:
    snac torch device = cuda NO FASTER than cpu (3080ti) (?!)
    speed is NO SLOWER using q4 vs q8 (llama-server, 3080ti, cuda) (?!)

model:
    leah voice quite a lot more stable than default tara

LLM notes
    "model": "google/gemini-2.0-flash-lite-001",
    "model": "deepseek/deepseek-chat-v3-0324",
