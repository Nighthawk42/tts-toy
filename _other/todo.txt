retroactively delete now-ignored file


after, "stop", log audio segment info should say "(stopped)" at end of message

doing command while printing out text is a no no :/

need time-to-first-audio printout

[2] text segmenter 
    test new version where you feed it an entire text block kind of thing
    then use it for direct mode too, and then kill old version

[2.5] add scrollbar

put LLM on main machine and benchmark times and gpu and cpu usage again

bug - printing feedback during streaming 

figure out something like 'resubmit last user input'

add 'time-to-first-audio' log message

add "has run app at least once" file marker

in direct mode, if input starts with "leah:", don't prepend current voice. maybe also chat mode as well.

------------------------------------
TODO
------------------------------------

re-add replay

streaming response
    
    on first streaming text response, do log message, "Got first text" kind of thing

    move new logic out of app, or smth 

style titlebar

make public

    add README, add github. add github link in menu text
    make demo video, add to README
    post to reddit maybe

test other llm models more and fine-tune sys prompt if needed

enum for send_ui_message

option to verbalize user prompt; 
two independent voice settings maybe, one for user prompt and one for assistant message

[1] doublecheck possible bug:
    some chat, !clear, and then:
    what was the last thing i asked                                                                          
    <chuckle> you just asked me what the last thing you asked was. 

[2] audio no longer outputs after buffer underflow 

[3] text chunker - add second pass where it combines short sentences that will fit. probably does not enhance quality.

[3] get runtime errors on control-c - ugh, or meh?

[3] missing logic: chat request > [direct] > say stuff > chat request completes and adds to audio queue

[3] add mode to style transformer where it takes in a single color and ignores all special tags. use for user input printout

[feature] stream llm content, and dynamically chunk text as the text comes in. test using slow LLM like deepseek
[feature] only print out text chunks as they are being spoken. and maybe tween the printing of the characters linearly based on full duration.
[feature] print out llm content synchronously but "highlight" each chunk as it's being spoken

------------------------------------
MISC NOTES, operational
------------------------------------

local orpheus llm server
    my llama-server command
        C:\sd\llama.cpp\llama-server.exe -m c:\sd\models\llm\Orpheus-3b-FT-Q8_0.gguf -ngl 99 -c 4096 --host 0.0.0.0 --temp 0.5
        C:\sd\llama.cpp\llama-server.exe -m c:\sd\models\llm\orpheus-3b-0.1-ft-q4_k_m.gguf -ngl 99 -c 4096 --host 0.0.0.0 --temp 0.5

    llama-server temp setting is overridden by request setting fyi, as you would expect

    lm studio seems to behave fine as well

single-file gguf version of the original hf model?

    had to run llama.cpp convert script from wsl (sentencepiece install error)

    python convert_hf_to_gguf.py /mnt/c/sd/models/llm/orpheus_full_temp --outfile /mnt/c/sd/models/llm/orpheus_full_temp/combined

    error: cannot find tokenizer.model

    simply copying tokenizer.json to tokenizer.model didn't cut it

    could not figure out :/

orpheus 'idempotency'

    temp 0 + top_p 0 = kind-of-almost. repeating same prompt multiple times helps as well.

performance notes:
    snac torch device = cuda NO FASTER than cpu (3080ti) (?!)
    speed is NO SLOWER using q4 vs q8 (llama-server, 3080ti, cuda) (?!)

    "model": "openai/chatgpt-4o-latest",




Examples: You can find many models here, including variations of Llama, Flan-T5, and others. Search for models with active "Deploy" or "Use in Space" options.